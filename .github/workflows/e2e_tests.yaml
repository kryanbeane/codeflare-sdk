# e2e tests workflow for CodeFlare-SDK
name: e2e

on:
  pull_request:
    branches:
      - main
      - "release-*"
      - ray-jobs-feature
      - kueue-integration
    paths-ignore:
      - "docs/**"
      - "**.adoc"
      - "**.md"
      - "LICENSE"

concurrency:
  group: ${{ github.head_ref }}-${{ github.workflow }}
  cancel-in-progress: true

env:
  CODEFLARE_OPERATOR_IMG: "quay.io/project-codeflare/codeflare-operator:dev"
  KUEUE_VERSION: "v0.13.4"

jobs:
  kubernetes:
    runs-on: gpu-t4-4-core

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Checkout Kuberay operator repository
        uses: actions/checkout@v4
        with:
          repository: ray-project/kuberay
          path: kuberay

      - name: Set Go
        uses: actions/setup-go@v5
        with:
          go-version-file: './kuberay/ray-operator/go.mod'
          cache-dependency-path: "./kuberay/ray-operator/go.sum"

      - name: Set up gotestfmt
        uses: gotesttools/gotestfmt-action@v2
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up specific Python version
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip" # caching pip dependencies

      - name: Setup NVidia GPU environment for KinD
        uses: ./.github/nvidia/nvidia-gpu-setup

      - name: Setup and start KinD cluster
        uses: ./.github/kind
        with:
          worker-nodes: 1

      - name: Install NVidia GPU operator for KinD
        uses: ./.github/nvidia/nvidia-gpu-operator

      - name: Wait for nodes to be ready
        run: |
          echo "Waiting for all nodes to be ready..."
          kubectl wait --for=condition=Ready nodes --all --timeout=300s

          echo "Checking node status..."
          kubectl get nodes -o wide

          echo "Checking for CNI readiness..."
          for i in {1..30}; do
            if kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.conditions[?(@.type=="Ready")].status}{"\n"}{end}' | grep -v "True"; then
              echo "Waiting for CNI to initialize (attempt $i/30)..."
              sleep 10
            else
              echo "All nodes are ready!"
              break
            fi
          done

          # Final verification
          kubectl describe nodes | grep -E "Ready|NetworkReady|RuntimeReady|PodCIDR"

      - name: Deploy Kuberay and Kueue
        id: deploy
        run: |
          echo Setting up Kuberay and Kueue
          make setup-e2e

      - name: Verify CodeFlare deployment
        run: |
          # Wait for Kueue to be ready
          echo "Waiting for Kueue controller to be ready..."
          kubectl wait --for=condition=Available --timeout=300s deployment -n kueue-system kueue-controller-manager || {
            echo "Kueue deployment status:"
            kubectl get all -n kueue-system
            exit 1
          }

          # Wait for KubeRay to be ready
          echo "Waiting for KubeRay operator to be ready..."
          kubectl wait --for=condition=Available --timeout=300s deployment -n default kuberay-operator || {
            echo "KubeRay deployment status:"
            kubectl get all -n default
            exit 1
          }

          # Check if CodeFlare operator is deployed
          if kubectl get deployment -n openshift-operators codeflare-operator-manager 2>/dev/null; then
            echo "CodeFlare operator found, checking webhook certificates..."
            kubectl get secret -n openshift-operators codeflare-operator-webhook-server-cert -o jsonpath='{.data.ca\.crt}' | base64 -d | openssl x509 -noout -text || {
              echo "Warning: Webhook certificate might be missing or invalid"
            }
          else
            echo "CodeFlare operator not found in openshift-operators namespace"
          fi

      - name: Add user to KinD
        uses: ./.github/kind/kind-add-user
        with:
          user-name: sdk-user

      - name: Configure RBAC for sdk user with limited permissions
        run: |
          kubectl create clusterrole list-ingresses --verb=get,list --resource=ingresses.networking.k8s.io
          kubectl create clusterrolebinding sdk-user-list-ingresses --clusterrole=list-ingresses --user=sdk-user
          kubectl create clusterrole namespace-creator --verb=get,list,create,delete,patch --resource=namespaces
          kubectl create clusterrolebinding sdk-user-namespace-creator --clusterrole=namespace-creator --user=sdk-user
          kubectl create clusterrole raycluster-creator --verb=get,list,create,delete,patch,watch,update --resource=rayclusters.ray.io,rayclusters/status
          kubectl create clusterrolebinding sdk-user-raycluster-creator --clusterrole=raycluster-creator --user=sdk-user
          kubectl create clusterrole rayjob-creator --verb=get,list,create,delete,patch,watch,update --resource=rayjobs.ray.io,rayjobs/status
          kubectl create clusterrolebinding sdk-user-rayjob-creator --clusterrole=rayjob-creator --user=sdk-user
          kubectl create clusterrole appwrapper-creator --verb=get,list,create,delete,patch,watch,update --resource=appwrappers.workload.codeflare.dev
          kubectl create clusterrolebinding sdk-user-appwrapper-creator --clusterrole=appwrapper-creator --user=sdk-user
          kubectl create clusterrole appwrapper-status-creator --verb=get,update,patch --resource=appwrappers/status --apiGroup=workload.codeflare.dev
          kubectl create clusterrolebinding sdk-user-appwrapper-status-creator --clusterrole=appwrapper-status-creator --user=sdk-user
          kubectl create clusterrole crd-viewer --verb=get,list,watch --resource=customresourcedefinitions.apiextensions.k8s.io
          kubectl create clusterrolebinding sdk-user-crd-viewer --clusterrole=crd-viewer --user=sdk-user
          kubectl create clusterrole resourceflavor-creator --verb=get,list,create,delete,patch --resource=resourceflavors.kueue.x-k8s.io
          kubectl create clusterrolebinding sdk-user-resourceflavor-creator --clusterrole=resourceflavor-creator --user=sdk-user
          kubectl create clusterrole clusterqueue-creator --verb=get,list,create,delete,patch --resource=clusterqueues.kueue.x-k8s.io
          kubectl create clusterrolebinding sdk-user-clusterqueue-creator --clusterrole=clusterqueue-creator --user=sdk-user
          kubectl create clusterrole localqueue-creator --verb=get,list,create,delete,patch --resource=localqueues.kueue.x-k8s.io
          kubectl create clusterrolebinding sdk-user-localqueue-creator --clusterrole=localqueue-creator --user=sdk-user
          kubectl create clusterrole workload-creator --verb=get,list,watch,update,patch --resource=workloads.kueue.x-k8s.io,workloads/status
          kubectl create clusterrolebinding sdk-user-workload-creator --clusterrole=workload-creator --user=sdk-user
          kubectl create clusterrole list-secrets --verb=get,list --resource=secrets
          kubectl create clusterrolebinding sdk-user-list-secrets --clusterrole=list-secrets --user=sdk-user
          kubectl create clusterrole pod-creator --verb=get,list,watch --resource=pods
          kubectl create clusterrolebinding sdk-user-pod-creator --clusterrole=pod-creator --user=sdk-user
          kubectl create clusterrole service-reader --verb=get,list,watch --resource=services
          kubectl create clusterrolebinding sdk-user-service-reader --clusterrole=service-reader --user=sdk-user
          kubectl create clusterrole port-forward-pods --verb=create --resource=pods/portforward
          kubectl create clusterrolebinding sdk-user-port-forward-pods-binding --clusterrole=port-forward-pods --user=sdk-user
          kubectl create clusterrole pod-logs --verb=get --resource=pods/log
          kubectl create clusterrolebinding sdk-user-pod-logs --clusterrole=pod-logs --user=sdk-user
          kubectl create clusterrole pod-exec --verb=create,get --resource=pods/exec
          kubectl create clusterrolebinding sdk-user-pod-exec --clusterrole=pod-exec --user=sdk-user
          kubectl create clusterrole node-reader --verb=get,list --resource=nodes
          kubectl create clusterrolebinding sdk-user-node-reader --clusterrole=node-reader --user=sdk-user
          kubectl create clusterrole event-creator --verb=get,list,create,patch --resource=events
          kubectl create clusterrolebinding sdk-user-event-creator --clusterrole=event-creator --user=sdk-user
          kubectl create clusterrole configmap-creator --verb=get,list,create,update,patch,delete --resource=configmaps
          kubectl create clusterrolebinding sdk-user-configmap-creator --clusterrole=configmap-creator --user=sdk-user
          kubectl create clusterrole batch-job-creator --verb=get,list,watch,create,update,patch,delete --resource=jobs.batch
          kubectl create clusterrolebinding sdk-user-batch-job-creator --clusterrole=batch-job-creator --user=sdk-user

          # Wait for RBAC to propagate
          echo "Waiting for RBAC permissions to propagate..."
          sleep 5

          # Verify CRDs are installed before switching context
          echo "Checking if AppWrapper CRD is installed..."
          kubectl get crd appwrappers.workload.codeflare.dev || echo "Warning: AppWrapper CRD not found"

          kubectl config use-context sdk-user

          # Verify permissions
          echo "Verifying RBAC permissions for sdk-user..."
          kubectl auth can-i list rayclusters.ray.io --all-namespaces || echo "Warning: Cannot list RayClusters"
          kubectl auth can-i list appwrappers.workload.codeflare.dev --all-namespaces || echo "Warning: Cannot list AppWrappers"
          kubectl auth can-i list customresourcedefinitions || echo "Warning: Cannot list CRDs"

      - name: Verify cluster readiness before tests
        run: |
          echo "=== Pre-test cluster verification ==="
          echo "Current context:"
          kubectl config current-context

          echo -e "\nCurrent user:"
          kubectl auth whoami || kubectl config view --minify -o jsonpath='{.users[0].name}'

          echo -e "\nNode status:"
          kubectl get nodes -o wide

          echo -e "\nSystem pods status:"
          kubectl get pods -A | grep -E "(kube-system|kueue-system|openshift-operators|default)" | grep -E "(kuberay|kueue|codeflare)" || true

          echo -e "\nChecking for any pods in error state:"
          kubectl get pods -A --field-selector=status.phase!=Running,status.phase!=Succeeded | grep -v "NAMESPACE" || echo "No pods in error state"

          echo -e "\nKueue resources:"
          kubectl get resourceflavors,clusterqueues,localqueues -A || true

          echo -e "\nRay CRDs:"
          kubectl get crd | grep ray || true

          echo -e "\nAll CRDs (checking for AppWrapper):"
          kubectl get crd | grep -E "(ray|appwrapper|workload)" || echo "No matching CRDs found"

      - name: Run e2e tests
        run: |
          export CODEFLARE_TEST_OUTPUT_DIR="${{ runner.temp }}/test-logs"
          mkdir -p ${CODEFLARE_TEST_OUTPUT_DIR}
          echo "CODEFLARE_TEST_OUTPUT_DIR=${CODEFLARE_TEST_OUTPUT_DIR}" >> $GITHUB_ENV

          set -euo pipefail
          pip install poetry
          poetry install --with test,docs
          echo "Running e2e tests..."
          poetry run pytest -v -s ./tests/e2e -m 'kind and nvidia_gpu' > ${CODEFLARE_TEST_OUTPUT_DIR}/e2e-pytest_output.log 2>&1
        env:
          GRPC_DNS_RESOLVER: "native"

      - name: Run RayJob e2e tests
        run: |
          set -euo pipefail
          echo "Running RayJob e2e tests..."
          # Set environment variable to prevent default queue assignment for non-Kueue tests
          export DISABLE_DEFAULT_KUEUE_QUEUE=true

          # Run only the tests that are designed for Kueue integration
          poetry run pytest -v -s ./tests/e2e/rayjob/ -x > ${CODEFLARE_TEST_OUTPUT_DIR}/rayjob_pytest_output.log 2>&1
        env:
          GRPC_DNS_RESOLVER: "native"

      - name: Switch to kind-cluster context to print logs
        if: always() && steps.deploy.outcome == 'success'
        run: kubectl config use-context kind-cluster

      - name: Print RayJob E2E Pytest output log
        if: always() && steps.deploy.outcome == 'success'
        run: |
          echo "Printing RayJob Pytest output logs"
          cat ${CODEFLARE_TEST_OUTPUT_DIR}/rayjob_pytest_output.log || echo "No RayJob test output found"

      - name: Print E2E Pytest output log
        if: always() && steps.deploy.outcome == 'success'
        run: |
          echo "Printing E2E Pytest output logs"
          cat ${CODEFLARE_TEST_OUTPUT_DIR}/e2e-pytest_output.log || echo "No E2E test output found"

      - name: Print CodeFlare operator logs
        if: always() && steps.deploy.outcome == 'success'
        run: |
          echo "Checking for CodeFlare operator logs"
          if kubectl get deployment -A codeflare-operator-manager 2>/dev/null; then
            kubectl logs -A --tail -1 -l app.kubernetes.io/name=codeflare-operator | tee ${CODEFLARE_TEST_OUTPUT_DIR}/codeflare-operator.log || echo "Failed to get CodeFlare operator logs"
          else
            echo "CodeFlare operator not deployed, skipping logs"
          fi

      - name: Print KubeRay operator logs
        if: always() && steps.deploy.outcome == 'success'
        run: |
          echo "Printing KubeRay operator logs"
          # Try default namespace first (where we verified it was deployed)
          kubectl logs -n default --tail -1 deployment/kuberay-operator | tee ${CODEFLARE_TEST_OUTPUT_DIR}/kuberay.log || \
          kubectl logs -n ray-system --tail -1 -l app.kubernetes.io/name=kuberay | tee ${CODEFLARE_TEST_OUTPUT_DIR}/kuberay.log || \
          echo "Failed to get KubeRay operator logs"

      - name: Export all KinD pod logs
        uses: ./.github/kind/kind-export-logs
        if: always() && steps.deploy.outcome == 'success'
        with:
          output-directory: ${{ env.CODEFLARE_TEST_OUTPUT_DIR }}

      - name: Upload logs
        uses: actions/upload-artifact@v4
        if: always() && steps.deploy.outcome == 'success'
        with:
          name: logs
          retention-days: 10
          path: |
            ${{ env.CODEFLARE_TEST_OUTPUT_DIR }}/**/*.log
          if-no-files-found: warn
